# -*- coding: utf-8 -*-

import json
import logging
import platform
import subprocess
import warnings

import psutil
import requests

from odoo import models, fields, api
from odoo.exceptions import UserError

_logger = logging.getLogger(__name__)

try:
    import GPUtil
except ImportError:
    GPUtil = None
    _logger.warning(
        "GPUtil no est√° instalado. La informaci√≥n de la GPU no estar√° disponible."
    )


class InstallationWizard(models.TransientModel):
    _name = "ai.installation.wizard"
    _description = "Asistente de Instalaci√≥n - Configuraci√≥n Autom√°tica"

    # Paso 1: Detecci√≥n de Ollama
    ollama_installed = fields.Boolean(string="Ollama Detectado", readonly=True)
    ollama_version = fields.Char(string="Versi√≥n de Ollama", readonly=True)
    ollama_status = fields.Selection(
        [
            ("not_installed", "No instalado"),
            ("installed", "Instalado"),
            ("running", "Ejecut√°ndose"),
        ],
        string="Estado de Ollama",
        readonly=True,
    )

    # Paso 2: Modelos disponibles
    available_models = fields.Text(string="Modelos Disponibles", readonly=True)
    selected_model = fields.Selection(
        selection="_get_dynamic_model_options", string="Modelo Instalado", default=None
    )

    def _get_dynamic_model_options(self):
        """Devuelve lista de tuplas con modelos instalados y remotos para el campo de selecci√≥n"""
        current_log = self.installation_log or ""  # Asegurarse de que sea una cadena
        options = []
        _logger.info(
            "Iniciando _get_dynamic_model_options para obtener opciones de modelos."
        )

        ram_total = self.ram_total

        recommendation_text = ""
        if ram_total:
            if ram_total < 8:
                recommendation_text = " (Recomendado para RAM < 8GB)"
            elif 8 <= ram_total < 16:
                recommendation_text = " (Recomendado para RAM 8-16GB)"
            else:
                recommendation_text = " (Recomendado para RAM > 16GB)"

        # Revalidar estado de Ollama y modelos al cargar opciones
        current_ollama_status = self._detect_ollama()["ollama_status"]
        if current_ollama_status == "running":
            _logger.info(
                "Ollama est√° ejecut√°ndose. Obteniendo modelos instalados localmente."
            )
            # Obtener modelos instalados localmente
            installed_models_data = self._get_available_models()
            _logger.info(
                "Modelos instalados localmente encontrados: %s",
                len(installed_models_data.get("models", [])),
            )
            for model in installed_models_data.get("models", []):
                try:
                    model_name = model.get("name")
                    model_details = model.get("details", {})
                    parameter_size = model_details.get("parameter_size")

                    if model_name and parameter_size:
                        options.append(
                            (
                                model_name,
                                f"{model_name} (Instalado - {parameter_size}){recommendation_text}",
                            )
                        )
                        _logger.info("A√±adido modelo instalado: %s", model_name)
                    else:
                        current_log += (
                            "‚ö†Ô∏è Modelo instalado con estructura inesperada: %s - %s\n"
                            % (model_name, model_details)
                        )
                        _logger.warning(
                            "Modelo instalado con estructura inesperada: %s - %s",
                            model_name,
                            model_details,
                        )
                except Exception as e:
                    model_label = model.get("name", "desconocido")
                    current_log += (
                        "‚ùå Error procesando modelo instalado %s: %s\n"
                        % (model_label, str(e))
                    )
                    _logger.error(
                        "Error procesando modelo instalado %s: %s", model_label, str(e)
                    )

            _logger.info("Obteniendo modelos remotos.")
            # Obtener modelos remotos
            remote_models_data = self._get_remote_models()
            _logger.info(
                "Modelos remotos encontrados: %s",
                len(remote_models_data.get("models", [])),
            )
            for model in remote_models_data.get("models", []):
                try:
                    model_name = model.get("name")
                    model_details = model.get("details", {})
                    parameter_size = model_details.get("parameter_size")

                    # Solo a√±adir modelos remotos que no est√©n ya instalados
                    if (
                        model_name
                        and parameter_size
                        and model_name not in [opt[0] for opt in options]
                    ):
                        options.append(
                            (
                                f"remote_{model_name}",
                                f"[Remoto] {model_name} ({parameter_size}){recommendation_text}",
                            )
                        )
                        _logger.info("A√±adido modelo remoto: %s", model_name)
                    elif model_name and parameter_size:
                        current_log += (
                            "‚ÑπÔ∏è Modelo remoto %s ya est√° instalado localmente, "
                            "no se a√±ade como opci√≥n remota.\n"
                            % model_name
                        )
                        _logger.info(
                            "Modelo remoto %s ya est√° instalado localmente, no se a√±ade como opci√≥n remota.",
                            model_name,
                        )
                    else:
                        current_log += (
                            "‚ö†Ô∏è Modelo remoto con estructura inesperada: %s - %s\n"
                            % (model_name, model_details)
                        )
                        _logger.warning(
                            "Modelo remoto con estructura inesperada: %s - %s",
                            model_name,
                            model_details,
                        )
                except Exception as e:
                    model_label = model.get("name", "desconocido")
                    current_log += (
                        "‚ùå Error procesando modelo remoto %s: %s\n"
                        % (model_label, str(e))
                    )
                    _logger.error(
                        "Error procesando modelo remoto %s: %s", model_label, str(e)
                    )

            self.write({"installation_log": current_log})  # Guardar log
            _logger.info(
                "Total de opciones de modelos generadas: %s", len(options)
            )
            if options:
                return options

        current_log += "‚ÑπÔ∏è No hay modelos disponibles (Ollama no est√° ejecut√°ndose o no hay conexi√≥n).\n"
        self.write({"installation_log": current_log})  # Guardar log
        _logger.info(
            "No hay modelos disponibles (Ollama no est√° ejecut√°ndose o no hay conexi√≥n)."
        )
        return [("none", "No hay modelos disponibles")]

    # Paso 3: Configuraci√≥n de Base de Datos
    db_configured = fields.Boolean(string="BD Configurada", readonly=True)
    vector_db_status = fields.Selection(
        [
            ("not_configured", "No configurado"),
            ("configured", "Configurado"),
            ("ready", "Listo"),
        ],
        string="Estado de Vector DB",
        readonly=True,
    )

    # Paso 4: Configuraci√≥n Avanzada (Proactividad)
    enable_watchdogs = fields.Boolean(
        string="Activar Watchdogs (Alertas Proactivas)", default=True
    )
    enable_dashboards = fields.Boolean(
        string="Activar Dashboards Ejecutivos", default=True
    )
    enable_actions = fields.Boolean(
        string="Permitir Ejecuci√≥n de Acciones Autom√°ticas", default=False
    )
    show_pending_actions = fields.Boolean(
        string="Mostrar Acciones Pendientes en Dashboard", default=True
    )
    advanced_config_saved = fields.Boolean(
        string="Configuraci√≥n Avanzada Guardada", readonly=True
    )

    # Estado del wizard
    current_step = fields.Selection(
        [
            ("detection", "Detecci√≥n"),
            ("model_selection", "Selecci√≥n de Modelo"),
            ("configuration", "Configuraci√≥n de BD"),
            ("advanced_config", "Configuraci√≥n Avanzada"),
            ("completion", "Completado"),
        ],
        string="Paso Actual",
        default="detection",
    )

    progress = fields.Integer(string="Progreso", default=0)
    installation_log = fields.Text(string="Log de Instalaci√≥n", readonly=True)

    # Informaci√≥n de Hardware
    cpu_count = fields.Integer(string="N√∫mero de CPUs", readonly=True)
    cpu_freq = fields.Float(string="Frecuencia de CPU (MHz)", readonly=True)
    ram_total = fields.Float(string="RAM Total (GB)", readonly=True)
    gpu_info = fields.Text(string="Informaci√≥n de GPU", readonly=True)

    # M√©todos de detecci√≥n autom√°tica
    @api.model
    def default_get(self, fields_list):
        res = super().default_get(fields_list)

        # Detectar estado de Ollama autom√°ticamente
        ollama_status = self._detect_ollama()
        res.update(ollama_status)

        # Verificar modelos disponibles
        if ollama_status["ollama_status"] == "running":
            available_models = self._get_available_models()
            res["available_models"] = json.dumps(available_models, indent=2)

        # Verificar configuraci√≥n de BD
        db_status = self._check_database_config()
        res.update(db_status)

        # Obtener informaci√≥n del sistema
        system_info = self._get_system_info()
        res.update(system_info)

        return res

    def _detect_ollama(self):
        """Detecta si Ollama est√° instalado y ejecut√°ndose"""
        result = {
            "ollama_installed": False,
            "ollama_version": "No detectado",
            "ollama_status": "not_installed",
            "installation_log": "Iniciando detecci√≥n de Ollama...\n",
        }

        current_log = self.installation_log or ""
        try:
            # Verificar si el comando ollama existe
            process = subprocess.run(
                ["ollama", "--version"],
                capture_output=True,
                text=True,
                timeout=10,
                check=False,
            )

            if process.returncode == 0:
                result["ollama_installed"] = True
                result["ollama_version"] = process.stdout.strip()
                current_log += f"‚úÖ Ollama detectado: {process.stdout.strip()}\n"

                # Verificar si el servicio est√° ejecut√°ndose
                try:
                    response = requests.get(
                        "http://localhost:11434/api/tags", timeout=5
                    )
                    if response.status_code == 200:
                        result["ollama_status"] = "running"
                        current_log += "‚úÖ Servicio Ollama ejecut√°ndose\n"
                    else:
                        result["ollama_status"] = "installed"
                        current_log += "‚ö†Ô∏è Ollama instalado pero no ejecut√°ndose\n"
                except requests.RequestException:
                    result["ollama_status"] = "installed"
                    current_log += "‚ö†Ô∏è Ollama instalado pero no ejecut√°ndose\n"

            else:
                current_log += "‚ùå Ollama no encontrado en el sistema\n"

        except (FileNotFoundError, subprocess.TimeoutExpired):
            current_log += "‚ùå Ollama no est√° instalado\n"

        result["installation_log"] = current_log
        return result

    def _get_available_models(self):
        """Obtiene lista de modelos disponibles en Ollama con registro de errores y tiempo de espera mayor"""
        current_log = self.installation_log or ""  # Asegurarse de que sea una cadena
        try:
            # Aumentar tiempo de espera para hardware m√°s lento (I5 2013)
            response = requests.get("http://localhost:11434/api/tags", timeout=20)
            if response.status_code == 200:
                models_data = response.json()
                model_names = [
                    model["name"] for model in models_data.get("models", [])
                ]
                current_log += f"üîç Modelos detectados: {model_names}\n"
                self.write({"installation_log": current_log})
                return models_data

            current_log += (
                "‚ùå Error en API de Ollama: C√≥digo de estado %s\n"
                % response.status_code
            )
            self.write({"installation_log": current_log})
        except requests.RequestException as e:
            current_log += f"‚ùå Fallo al obtener modelos: {str(e)}\n"
            self.write({"installation_log": current_log})  # Guardar log
        return {"models": []}

    def _get_remote_models(self):
        """Devuelve una lista curada de modelos populares recomendados para descargar"""
        current_log = self.installation_log or ""

        # Lista curada de modelos populares y eficientes
        # Incluye nombre, y tama√±o aproximado en GB para referencia
        curated_models = [
            {"name": "llama3.2:1b", "details": {"parameter_size": "1.3GB"}},
            {"name": "llama3.2:3b", "details": {"parameter_size": "2.0GB"}},
            {"name": "deepseek-r1:1.5b", "details": {"parameter_size": "1.1GB"}},
            {"name": "deepseek-r1:7b", "details": {"parameter_size": "4.7GB"}},
            {"name": "deepseek-r1:8b", "details": {"parameter_size": "4.9GB"}},
            {"name": "phi3:mini", "details": {"parameter_size": "2.3GB"}},
            {"name": "gemma2:2b", "details": {"parameter_size": "1.6GB"}},
            {"name": "gemma2:9b", "details": {"parameter_size": "5.4GB"}},
            {"name": "qwen2.5:0.5b", "details": {"parameter_size": "394MB"}},
            {"name": "qwen2.5:1.5b", "details": {"parameter_size": "986MB"}},
            {"name": "qwen2.5:3b", "details": {"parameter_size": "1.9GB"}},
            {"name": "qwen2.5:7b", "details": {"parameter_size": "4.7GB"}},
            {"name": "mistral", "details": {"parameter_size": "4.1GB"}},
            {"name": "neural-chat", "details": {"parameter_size": "4.1GB"}},
            {"name": "starling-lm", "details": {"parameter_size": "4.1GB"}},
        ]

        current_log += (
            f"üåê Cargando lista de {len(curated_models)} modelos recomendados...\n"
        )
        self.write({"installation_log": current_log})

        return {"models": curated_models}

    def _check_database_config(self):
        """Verifica la configuraci√≥n de la base de datos"""
        # Esta es una verificaci√≥n b√°sica - se expandir√° con la integraci√≥n de vector DB
        return {
            "db_configured": True,  # Odoo ya tiene su BD configurada
            "vector_db_status": "not_configured",
        }

    def _get_system_info(self):
        """Obtiene informaci√≥n del sistema (CPU, RAM, GPU)"""
        current_log = self.installation_log or ""
        system_info = {}

        try:
            # CPU Info
            system_info["cpu_count"] = psutil.cpu_count(logical=True)
            system_info["cpu_freq"] = psutil.cpu_freq().current
            current_log += (
                "üíª CPU: %s cores, %.2f MHz\n"
                % (system_info["cpu_count"], system_info["cpu_freq"])
            )

            # RAM Info
            svmem = psutil.virtual_memory()
            system_info["ram_total"] = round(svmem.total / (1024**3), 2)  # GB
            current_log += "üíæ RAM Total: %.2f GB\n" % system_info["ram_total"]

            # GPU Info
            if GPUtil:
                try:
                    with warnings.catch_warnings():
                        warnings.filterwarnings("ignore", category=DeprecationWarning)
                        gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu_info_list = []
                        for gpu in gpus:
                            gpu_info_list.append(
                                {
                                    "id": gpu.id,
                                    "name": gpu.name,
                                    "memoryTotal": gpu.memoryTotal,
                                    "memoryUsed": gpu.memoryUsed,
                                    "memoryFree": gpu.memoryFree,
                                    "driver": gpu.driver,
                                    "temperature": gpu.temperature,
                                    "utilization": gpu.load * 100,
                                }
                            )
                            current_log += (
                                "üéÆ GPU %s: %s (%sMB, %.1f%% util)\n"
                                % (
                                    gpu.id,
                                    gpu.name,
                                    gpu.memoryTotal,
                                    gpu.load * 100,
                                )
                            )
                        system_info["gpu_info"] = json.dumps(gpu_info_list)
                    else:
                        system_info["gpu_info"] = "No GPU detected"
                        current_log += "üéÆ GPU: No GPU detectada\n"
                except Exception as gpu_e:
                    system_info["gpu_info"] = "Error getting GPU info: %s" % str(
                        gpu_e
                    )
                    current_log += (
                        "‚ùå Error obteniendo informaci√≥n de GPU: %s\n" % str(gpu_e)
                    )
                    _logger.warning(
                        "GPUtil error: %s. It might not be installed or no GPUs are present.",
                        gpu_e,
                    )
            else:
                system_info["gpu_info"] = (
                    "GPUtil no est√° disponible, informaci√≥n de GPU no detectada."
                )
                current_log += "üéÆ GPU: GPUtil no est√° disponible, informaci√≥n de GPU no detectada.\n"

        except Exception as e:
            current_log += f"‚ùå Error obteniendo informaci√≥n del sistema: {str(e)}\n"
            _logger.error("Error getting system info: %s", e)

        self.write({"installation_log": current_log})
        return system_info

    # Acciones del wizard
    def action_install_ollama(self):
        """Intenta instalar Ollama autom√°ticamente"""
        current_log = self.installation_log or ""  # Asegurarse de que sea una cadena
        current_log += "üöÄ Intentando instalar Ollama autom√°ticamente...\n"

        try:
            # Detectar sistema operativo
            system = platform.system().lower()

            if system == "windows":
                # Para Windows - descargar instalador
                current_log += "üì• Descargando instalador para Windows...\n"
                # En una implementaci√≥n real, descargar√≠amos el instalador
                current_log += "‚úÖ Por favor, descarga Ollama desde https://ollama.ai/download y ejecuta el instalador\n"

            elif system == "linux":
                # Para Linux - instalar via curl
                current_log += "üêß Instalando Ollama en Linux...\n"
                try:
                    process = subprocess.run(
                        ["curl", "-fsSL", "https://ollama.ai/install.sh"],
                        capture_output=True,
                        text=True,
                        timeout=30,
                        check=False,
                    )
                    if process.returncode == 0:
                        install_script = process.stdout
                        # Ejecutar script de instalaci√≥n
                        install_process = subprocess.run(
                            ["sh", "-c", install_script],
                            capture_output=True,
                            text=True,
                            timeout=120,
                            check=False,
                        )
                        current_log += install_process.stdout
                        if install_process.returncode == 0:
                            current_log += (
                                "‚úÖ Ollama instalado correctamente en Linux\n"
                            )
                        else:
                            current_log += (
                                f"‚ùå Error en instalaci√≥n: {install_process.stderr}\n"
                            )

                except subprocess.TimeoutExpired:
                    current_log += "‚ùå Timeout durante la instalaci√≥n\n"

            elif system == "darwin":  # macOS
                current_log += "üçé Para macOS: brew install ollama\n"

            # Actualizar estado despu√©s de intentar instalar
            new_status = self._detect_ollama()
            self.write(new_status)
            self.write({"installation_log": current_log})  # Guardar log actualizado

        except Exception as e:
            current_log += f"‚ùå Error durante instalaci√≥n: {str(e)}\n"
            self.write({"installation_log": current_log})  # Guardar log de error

        return self._show_installation_view()

    def action_download_model(self):
        """Descarga el modelo seleccionado"""
        current_log = self.installation_log or ""  # Asegurarse de que sea una cadena

        if not self.selected_model:
            current_log += (
                "‚ùå Error: No se ha seleccionado ning√∫n modelo para descargar.\n"
            )
            self.write({"installation_log": current_log})
            raise UserError(
                self.env._("Por favor, selecciona un modelo para descargar.")
            )

        model_to_download = str(self.selected_model)  # Asegurarse de que sea una cadena
        if model_to_download.startswith("remote_"):
            model_to_download = model_to_download[len("remote_") :]
            current_log += f"üì• Descargando modelo remoto {model_to_download}...\n"
        else:
            current_log += (
                f"üì• Verificando/actualizando modelo local {model_to_download}...\n"
            )

        try:
            process = subprocess.run(
                ["ollama", "pull", model_to_download],
                capture_output=True,
                text=True,
                timeout=300,
                check=False,
            )  # 5 minutos timeout

            current_log += process.stdout
            if process.returncode == 0:
                current_log += f"‚úÖ Modelo {model_to_download} descargado/actualizado correctamente\n"
                # Auto-seleccionar el modelo reci√©n descargado para facilitar el avance
                self.selected_model = model_to_download
            else:
                current_log += f"‚ùå Error descargando/actualizando modelo {model_to_download}: {process.stderr}\n"

            # Actualizar lista de modelos disponibles
            available_models = self._get_available_models()
            self.write(
                {
                    "available_models": json.dumps(available_models, indent=2),
                    "installation_log": current_log,
                }
            )  # Guardar log y modelos

        except subprocess.TimeoutExpired:
            current_log += "‚ùå Timeout durante la descarga del modelo\n"
            self.write({"installation_log": current_log})  # Guardar log de error
        except Exception as e:
            current_log += f"‚ùå Error: {str(e)}\n"
            self.write({"installation_log": current_log})  # Guardar log de error

        return self._show_installation_view()

    def action_start_ollama(self):
        """Inicia el servicio Ollama"""
        current_log = self.installation_log or ""  # Asegurarse de que sea una cadena
        current_log += "üöÄ Iniciando servicio Ollama...\n"

        try:
            # Intentar iniciar Ollama (depende del SO)
            system = platform.system().lower()

            if system == "windows":
                # En Windows, Ollama se ejecuta como servicio autom√°ticamente
                process = subprocess.run(
                    ["ollama", "serve"],
                    capture_output=True,
                    text=True,
                    timeout=10,
                    check=False,
                )
                current_log += process.stdout

            elif system in ["linux", "darwin"]:
                process = subprocess.run(
                    ["systemctl", "start", "ollama"],
                    capture_output=True,
                    text=True,
                    timeout=30,
                    check=False,
                )
                current_log += process.stdout

            # Verificar estado
            new_status = self._detect_ollama()
            self.write(
                {**new_status, "installation_log": current_log}
            )  # Guardar log y nuevo estado

        except Exception as e:
            current_log += f"‚ùå Error iniciando Ollama: {str(e)}\n"
            self.write({"installation_log": current_log})  # Guardar log de error

        return self._show_installation_view()

    def action_configure_database(self):
        """Configura la base de datos vectorial"""
        current_log = self.installation_log or ""  # Asegurarse de que sea una cadena
        current_log += "üîß Configurando base de datos vectorial...\n"

        try:
            # Aqu√≠ ir√≠a la configuraci√≥n real de la vector DB
            # Por ahora, simulamos una configuraci√≥n exitosa
            self.write(
                {
                    "vector_db_status": "configured",
                    "installation_log": current_log
                    + "‚úÖ Base de datos vectorial configurada\n",
                }
            )

        except Exception as e:
            current_log += f"‚ùå Error configurando BD: {str(e)}\n"
            self.write({"installation_log": current_log})  # Guardar log de error

        return self._show_installation_view()

    def action_save_advanced_config(self):
        """Guarda la configuraci√≥n avanzada"""
        current_log = self.installation_log or ""
        current_log += "‚öôÔ∏è Guardando configuraci√≥n avanzada...\n"

        self.write(
            {
                "advanced_config_saved": True,
                "installation_log": current_log
                + "‚úÖ Configuraci√≥n avanzada guardada\n",
            }
        )
        return self._show_installation_view()

    def action_next_step(self):
        """Avanza al siguiente paso del wizard"""
        current_log = self.installation_log or ""  # Asegurarse de que sea una cadena
        current_log += "‚è≠Ô∏è Avanzando al siguiente paso...\n"

        try:
            if self.current_step == "detection":
                # Verificar que Ollama est√© ejecut√°ndose antes de avanzar
                if self.ollama_status != "running":
                    raise UserError(
                        self.env._("Ollama debe estar ejecut√°ndose para continuar")
                    )

                self.write(
                    {
                        "current_step": "model_selection",
                        "progress": 25,
                        "installation_log": current_log,
                    }
                )

            elif self.current_step == "model_selection":
                # Verificar que se haya seleccionado un modelo
                if not self.selected_model:
                    raise UserError(
                        self.env._("Debes seleccionar un modelo para continuar")
                    )

                self.write(
                    {
                        "current_step": "configuration",
                        "progress": 50,
                        "installation_log": current_log,
                    }
                )

            elif self.current_step == "configuration":
                # Verificar que la BD est√© configurada
                if self.vector_db_status not in ["configured", "ready"]:
                    raise UserError(
                        self.env._(
                            "La base de datos debe estar configurada para continuar"
                        )
                    )

                self.write(
                    {
                        "current_step": "advanced_config",
                        "progress": 75,
                        "installation_log": current_log,
                    }
                )

            elif self.current_step == "advanced_config":
                self.write(
                    {
                        "current_step": "completion",
                        "progress": 100,
                        "installation_log": current_log,
                    }
                )

        except Exception as e:
            current_log += f"‚ùå Error avanzando al siguiente paso: {str(e)}\n"
            self.write({"installation_log": current_log})  # Guardar log de error
            raise UserError(
                self.env._("Error avanzando al siguiente paso: %s") % str(e)
            ) from e

        return self._show_installation_view()

    def action_previous_step(self):
        """Retrocede al paso anterior del wizard"""
        current_log = self.installation_log or ""  # Asegurarse de que sea una cadena
        current_log += "‚èÆÔ∏è Retrocediendo al paso anterior...\n"

        try:
            if self.current_step == "model_selection":
                self.write(
                    {
                        "current_step": "detection",
                        "progress": 0,
                        "installation_log": current_log,
                    }
                )

            elif self.current_step == "configuration":
                self.write(
                    {
                        "current_step": "model_selection",
                        "progress": 25,
                        "installation_log": current_log,
                    }
                )

            elif self.current_step == "advanced_config":
                self.write(
                    {
                        "current_step": "configuration",
                        "progress": 50,
                        "installation_log": current_log,
                    }
                )

            elif self.current_step == "completion":
                self.write(
                    {
                        "current_step": "advanced_config",
                        "progress": 75,
                        "installation_log": current_log,
                    }
                )

        except Exception as e:
            current_log += f"‚ùå Error retrocediendo al paso anterior: {str(e)}\n"
            self.write({"installation_log": current_log})  # Guardar log de error
            raise UserError(
                self.env._("Error retrocediendo al paso anterior: %s") % str(e)
            ) from e

        return self._show_installation_view()

    def action_complete_installation(self):
        """Completa la instalaci√≥n y configura todo"""
        current_log = self.installation_log or ""  # Asegurarse de que sea una cadena
        current_log += "üéâ Completando instalaci√≥n...\n"

        try:
            # Verificar que todo est√© configurado
            if self.ollama_status != "running":
                raise UserError(
                    self.env._(
                        "Ollama debe estar ejecut√°ndose para completar la instalaci√≥n"
                    )
                )

            # 1. Guardar configuraci√≥n de servidor Ollama
            config = self.env["ai.ollama.config"].search([], limit=1)
            if not config:
                self.env["ai.ollama.config"].create(
                    {
                        "name": "Local Ollama",
                        "url": "http://localhost:11434",
                        "active": True,
                    }
                )
                current_log += "‚úÖ Configuraci√≥n del servidor Ollama guardada\n"

            # 2. Registrar el modelo seleccionado
            if self.selected_model:
                model_name = self.selected_model
                # Buscar si ya existe
                ollama_model = self.env["ai.ollama.model"].search(
                    [("name", "=", model_name)], limit=1
                )
                if not ollama_model:
                    self.env["ai.ollama.model"].create(
                        {"name": model_name, "display_name": model_name, "active": True}
                    )
                    current_log += f"‚úÖ Modelo {model_name} registrado en el cat√°logo\n"
                else:
                    ollama_model.write({"active": True})
                    current_log += f"‚úÖ Modelo {model_name} activado en el cat√°logo\n"

                # 3. Guardar como preferencia del sistema
                self.env["ir.config_parameter"].sudo().set_param(
                    "ai_production_assistant.selected_model", model_name
                )
                current_log += (
                    f"‚úÖ Modelo {model_name} establecido como predeterminado\n"
                )

            # 4. Guardar configuraci√≥n avanzada
            set_param = self.env["ir.config_parameter"].sudo().set_param
            set_param(
                "ai_production_assistant.enable_watchdogs", str(self.enable_watchdogs)
            )
            set_param(
                "ai_production_assistant.enable_dashboards", str(self.enable_dashboards)
            )
            set_param(
                "ai_production_assistant.enable_actions", str(self.enable_actions)
            )
            set_param(
                "ai_production_assistant.show_pending_actions",
                str(self.show_pending_actions),
            )
            current_log += "‚úÖ Preferencias de proactividad guardadas\n"

            # Configurar modelos por defecto si es necesario
            current_log += "‚úÖ Instalaci√≥n completada correctamente\n"
            self.write(
                {
                    "current_step": "completion",
                    "progress": 100,
                    "vector_db_status": "ready",
                    "installation_log": current_log,
                }
            )

        except Exception as e:
            current_log += f"‚ùå Error completando instalaci√≥n: {str(e)}\n"
            self.write({"installation_log": current_log})
            raise UserError(
                self.env._("Error completando instalaci√≥n: %s") % str(e)
            ) from e

        return self._show_installation_view()

    def action_installation_wizard(self):
        """
        Abre el wizard de instalaci√≥n.
        """
        return {
            "name": self.env._(
                "Asistente de Instalaci√≥n de AI Production Assistant"
            ),
            "type": "ir.actions.act_window",
            "res_model": "ai.installation.wizard",
            "view_mode": "form",
            "view_id": self.env.ref(
                "ai_production_assistant.view_installation_wizard_form"
            ).id,
            "target": "new",
            "res_id": self.create({}).id,
        }

    def _show_installation_view(self):
        """Muestra la vista actual del wizard"""
        return {
            "type": "ir.actions.act_window",
            "res_model": self._name,
            "res_id": self.id,
            "view_mode": "form",
            "target": "new",
            "context": self.env.context,
        }
